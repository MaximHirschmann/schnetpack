{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "schnetpack_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(1, schnetpack_dir + \"\\\\src\")\n",
    "\n",
    "import schnetpack as spk\n",
    "from schnetpack.data import ASEAtomsData\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Split file was given, but `num_val (10000) != len(val_idx)` (0)!\n",
      "100%|██████████| 10000/10000 [05:10<00:00, 32.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from schnetpack.datasets import rMD17\n",
    "from schnetpack.transform import ASENeighborList\n",
    "\n",
    "filepath_db = os.path.join(os.getcwd(), 'data\\\\rMD17\\\\rMD17.db')\n",
    "#filepath_split = os.path.join(os.getcwd(), 'data\\\\rMD17\\\\split_qm9.npz')\n",
    "\n",
    "ethanol_data = rMD17(\n",
    "    filepath_db, \n",
    "    molecule='ethanol',\n",
    "    batch_size=10,\n",
    "    num_train=100_000,\n",
    "    num_val=10_000,\n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=5.),\n",
    "        trn.RemoveOffsets(rMD17.energy, remove_mean=True, remove_atomrefs=False),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    num_workers=1,\n",
    "    pin_memory=True, # set to false, when not using a GPU\n",
    ")\n",
    "\n",
    "\n",
    "ethanol_data.prepare_data()\n",
    "ethanol_data.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run define_hessian_database.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:14<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "filepath_hessian_db = os.path.join(os.getcwd(), 'data\\\\ene_grad_hess_1000eth\\\\data.db')\n",
    "filepath_no_hessian_db = os.path.join(os.getcwd(), 'data\\\\ene_grad_hess_1000eth\\\\data-no-hessian.db')\n",
    "\n",
    "hessianData = spk.data.AtomsDataModule(\n",
    "    filepath_hessian_db, \n",
    "    distance_unit=\"Ang\",\n",
    "    property_units={\"energy\": \"Hartree\",\n",
    "                    \"forces\": \"Hartree/Bohr\",\n",
    "                    \"hessian\": \"Hartree/Bohr/Bohr\"\n",
    "                    },\n",
    "    batch_size=10,\n",
    "    \n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=5.),\n",
    "        trn.RemoveOffsets(\"energy\", remove_mean=True, remove_atomrefs=False),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    \n",
    "    num_train=800,\n",
    "    num_val=100,\n",
    "    num_test=100,\n",
    "    \n",
    "    pin_memory=True, # set to false, when not using a GPU\n",
    "    \n",
    ")\n",
    "hessianData.prepare_data()\n",
    "hessianData.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reference calculations: 1000\n",
      "Number of train data: 800\n",
      "Number of validation data: 100\n",
      "Number of test data: 100\n",
      "Available properties:\n",
      "- energy\n",
      "- forces\n",
      "- hessian\n"
     ]
    }
   ],
   "source": [
    "print('Number of reference calculations:', len(hessianData.dataset))\n",
    "print('Number of train data:', len(hessianData.train_dataset))\n",
    "print('Number of validation data:', len(hessianData.val_dataset))\n",
    "print('Number of test data:', len(hessianData.test_dataset))\n",
    "print('Available properties:')\n",
    "\n",
    "for p in hessianData.dataset.available_properties:\n",
    "    print('-', p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:199: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "WARNING:tensorflow:From c:\\Users\\Maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                   | Params\n",
      "---------------------------------------------------\n",
      "0 | model   | NeuralNetworkPotential | 43.1 K\n",
      "1 | outputs | ModuleList             | 0     \n",
      "---------------------------------------------------\n",
      "43.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "43.1 K    Total params\n",
      "0.173     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 80/80 [00:54<00:00,  1.47it/s, v_num=14, val_loss=0.000433]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 80/80 [00:54<00:00,  1.46it/s, v_num=14, val_loss=0.000433]\n"
     ]
    }
   ],
   "source": [
    "cutoff = 5.\n",
    "n_atom_basis = 30\n",
    "\n",
    "pairwise_distance = spk.atomistic.PairwiseDistances() # calculates pairwise distances between atoms\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)\n",
    "paiNN = spk.representation.PaiNN(\n",
    "    n_atom_basis=n_atom_basis, \n",
    "    n_interactions=3,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff)\n",
    ")\n",
    "\n",
    "pred_energy = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key=\"energy\")\n",
    "pred_forces = spk.atomistic.Forces(energy_key=\"energy\", force_key=\"forces\")\n",
    "pred_polarizability = spk.atomistic.Polarizability(n_in = n_atom_basis, polarizability_key = \"polarizability\")\n",
    "\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=paiNN,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_energy, pred_forces, pred_polarizability],\n",
    "    postprocessors=[\n",
    "        trn.CastTo64(),\n",
    "        trn.AddOffsets(\"energy\", add_mean=True, add_atomrefs=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_energy = spk.task.ModelOutput(\n",
    "    name=\"energy\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=0.01,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "\n",
    "output_forces = spk.task.ModelOutput(\n",
    "    name=\"forces\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=0.99,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "\n",
    "output_polarizability = spk.task.ModelOutput(\n",
    "    name=\"polarizability\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=0,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_energy, output_forces],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-4}\n",
    ")\n",
    "\n",
    "directory_training = os.path.join(os.getcwd(), \"maxim\\\\data\\\\ene_grad_hess_1000eth\")\n",
    "filepath_model = os.path.join(directory_training, \"best_inference_model\")\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=directory_training)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=filepath_model,\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=directory_training,\n",
    "    max_epochs=5, # for testing, we restrict the number of epochs\n",
    ")\n",
    "\n",
    "trainer.fit(task, datamodule=hessianData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'forces': tensor([[ 0.0153,  0.0014, -0.0916],\n",
      "        [-0.0186,  0.0672,  0.0408],\n",
      "        [ 0.0394,  0.0225, -0.0069],\n",
      "        [ 0.0048,  0.0030,  0.0374],\n",
      "        [-0.0145,  0.0242,  0.0312],\n",
      "        [-0.0271, -0.0990,  0.0203],\n",
      "        [ 0.0750, -0.0212, -0.0069],\n",
      "        [-0.0466,  0.0160, -0.0440],\n",
      "        [-0.0278, -0.0142,  0.0196]], dtype=torch.float64,\n",
      "       grad_fn=<ToCopyBackward0>), 'polarizability': tensor([[[-0.1886, -0.0071,  0.0048],\n",
      "         [-0.0071, -0.1638,  0.0053],\n",
      "         [ 0.0048,  0.0053, -0.1917]]], dtype=torch.float64,\n",
      "       grad_fn=<ToCopyBackward0>), 'energy': tensor([-154.6937], dtype=torch.float64, grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "\n",
    "# set device\n",
    "#device = torch.device(\"cuda\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# load model\n",
    "best_model = torch.load(filepath_model, map_location=device)\n",
    "\n",
    "# set up converter\n",
    "converter = spk.interfaces.AtomsConverter(\n",
    "    neighbor_list=trn.ASENeighborList(cutoff=5.0), dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "\n",
    "# create atoms object from dataset\n",
    "structure = hessianData.test_dataset[0]\n",
    "atoms = Atoms(\n",
    "    numbers=structure[spk.properties.Z], positions=structure[spk.properties.R]\n",
    ")\n",
    "\n",
    "# convert atoms to SchNetPack inputs and perform prediction\n",
    "inputs = converter(atoms)\n",
    "results = best_model(inputs)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<schnetpack.data.atoms.ASEAtomsData object at 0x000001490E82D690>\n"
     ]
    }
   ],
   "source": [
    "temp = hessianData\n",
    "print(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
