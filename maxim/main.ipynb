{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import schnetpack as spk\n",
    "from schnetpack.data import ASEAtomsData\n",
    "import schnetpack.transform as trn\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Split file was given, but `num_val (10000) != len(val_idx)` (0)!\n",
      "100%|██████████| 10000/10000 [05:10<00:00, 32.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from schnetpack.datasets import rMD17\n",
    "from schnetpack.transform import ASENeighborList\n",
    "\n",
    "filepath_db = os.path.join(os.getcwd(), 'data\\\\rMD17\\\\rMD17.db')\n",
    "#filepath_split = os.path.join(os.getcwd(), 'data\\\\rMD17\\\\split_qm9.npz')\n",
    "\n",
    "ethanol_data = rMD17(\n",
    "    filepath_db, \n",
    "    molecule='ethanol',\n",
    "    batch_size=10,\n",
    "    num_train=100_000,\n",
    "    num_val=10_000,\n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=5.),\n",
    "        trn.RemoveOffsets(rMD17.energy, remove_mean=True, remove_atomrefs=False),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    num_workers=1,\n",
    "    pin_memory=True, # set to false, when not using a GPU\n",
    ")\n",
    "\n",
    "\n",
    "ethanol_data.prepare_data()\n",
    "ethanol_data.setup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run define_hessian_database.py here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:11<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "filepath_hessian_db = os.path.join(os.getcwd(), 'data\\\\ene_grad_hess_1000eth\\\\data.db')\n",
    "filepath_no_hessian_db = os.path.join(os.getcwd(), 'data\\\\ene_grad_hess_1000eth\\\\data-no-hessian.db')\n",
    "\n",
    "hessianData = spk.data.AtomsDataModule(\n",
    "    filepath_hessian_db, \n",
    "    distance_unit=\"Ang\",\n",
    "    property_units={\"energy\": \"Hartree\",\n",
    "                    \"forces\": \"Hartree/Bohr\",\n",
    "                    \"hessian\": \"Hartree/Bohr/Bohr\"\n",
    "                    },\n",
    "    batch_size=10,\n",
    "    \n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=5.),\n",
    "        trn.RemoveOffsets(\"energy\", remove_mean=True, remove_atomrefs=False),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    \n",
    "    num_train=800,\n",
    "    num_val=100,\n",
    "    num_test=100,\n",
    "    \n",
    "    #pin_memory=True, # set to false, when not using a GPU\n",
    ")\n",
    "hessianData.prepare_data()\n",
    "hessianData.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100000\n",
      "100000\n",
      "[14155, 77555, 45811, 39209, 26021, 40902, 74535, 71920, 62531, 37630]\n"
     ]
    }
   ],
   "source": [
    "print(len(hessianData.dataset))\n",
    "print(len(hessianData.train_dataset))\n",
    "print(len(hessianData.train_idx))\n",
    "print(hessianData.train_idx[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reference calculations: 1000\n",
      "Number of train data: 800\n",
      "Number of validation data: 100\n",
      "Number of test data: 100\n",
      "Available properties:\n",
      "- energy\n",
      "- forces\n",
      "- hessian\n"
     ]
    }
   ],
   "source": [
    "print('Number of reference calculations:', len(hessianData.dataset))\n",
    "print('Number of train data:', len(hessianData.train_dataset))\n",
    "print('Number of validation data:', len(hessianData.val_dataset))\n",
    "print('Number of test data:', len(hessianData.test_dataset))\n",
    "print('Available properties:')\n",
    "\n",
    "for p in hessianData.dataset.available_properties:\n",
    "    print('-', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- energy\n",
      "- forces\n",
      "- hessian\n"
     ]
    }
   ],
   "source": [
    "for p in hessianData.property_units.keys():\n",
    "    print('-', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<schnetpack.data.loader.AtomsLoader at 0x22149fcb610>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessianData.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:199: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type                   | Params\n",
      "---------------------------------------------------\n",
      "0 | model   | NeuralNetworkPotential | 43.1 K\n",
      "1 | outputs | ModuleList             | 0     \n",
      "---------------------------------------------------\n",
      "43.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "43.1 K    Total params\n",
      "0.172     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae47a2dcf864e66a5964697142c6912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 10. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5996a3562a457fa4f1ca099667a56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc44ef97ace4080bfcf8845bd79c7c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae35ac85ff24feb8207f4821ec5f109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec7ca6f4aff4ce3ac36fcb92df50651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548bc9e9452c41e6beed31b99f5a4250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55de684f1a09462e8e7aae083ac7ffc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "cutoff = 5.\n",
    "n_atom_basis = 30\n",
    "\n",
    "pairwise_distance = spk.atomistic.PairwiseDistances() # calculates pairwise distances between atoms\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)\n",
    "paiNN = spk.representation.PaiNN(\n",
    "    n_atom_basis=n_atom_basis, \n",
    "    n_interactions=3,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff)\n",
    ")\n",
    "\n",
    "pred_energy = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key=\"energy\")\n",
    "pred_forces = spk.atomistic.Forces(energy_key=\"energy\", force_key=\"forces\")\n",
    "pred_polarizability = spk.atomistic.Polarizability(n_in = n_atom_basis, polarizability_key = \"polarizability\")\n",
    "\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=paiNN,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_energy, pred_forces, pred_polarizability],\n",
    "    postprocessors=[\n",
    "        trn.CastTo64(),\n",
    "        trn.AddOffsets(\"energy\", add_mean=True, add_atomrefs=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_energy = spk.task.ModelOutput(\n",
    "    name=\"energy\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=0.01,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "\n",
    "output_forces = spk.task.ModelOutput(\n",
    "    name=\"forces\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=0.99,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "\n",
    "output_polarizability = spk.task.ModelOutput(\n",
    "    name=\"polarizability\",\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=0,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_energy, output_forces],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-4}\n",
    ")\n",
    "\n",
    "directory_training = os.path.join(os.getcwd(), \"maxim\\\\data\\\\ene_grad_hess_1000eth\")\n",
    "filepath_model = os.path.join(directory_training, \"best_inference_model\")\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(save_dir=directory_training)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        model_path=filepath_model,\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=directory_training,\n",
    "    max_epochs=5, # for testing, we restrict the number of epochs\n",
    ")\n",
    "trainer.fit(task, datamodule=hessianData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': tensor([-97078.5350], dtype=torch.float64, grad_fn=<AddBackward0>), 'forces': tensor([[  7.2651,  57.8519, -58.1546],\n",
      "        [ -9.6054,   2.3938,  28.5954],\n",
      "        [ 14.8588, -39.4147,  87.0120],\n",
      "        [ 11.8051,  -9.7743,  -2.4411],\n",
      "        [ -1.5639,  -6.6200,   3.7865],\n",
      "        [ -4.2221, -12.5190,  11.4114],\n",
      "        [  2.5417,  -3.6735,  -3.0063],\n",
      "        [ 12.2374,   7.4088, -21.9501],\n",
      "        [-33.3167,   4.3472, -45.2533]], dtype=torch.float64,\n",
      "       grad_fn=<ToCopyBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "\n",
    "# set device\n",
    "#device = torch.device(\"cuda\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# load model\n",
    "best_model = torch.load(filepath_model, map_location=device)\n",
    "\n",
    "# set up converter\n",
    "converter = spk.interfaces.AtomsConverter(\n",
    "    neighbor_list=trn.ASENeighborList(cutoff=5.0), dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "\n",
    "# create atoms object from dataset\n",
    "structure = ethanol_data.test_dataset[0]\n",
    "atoms = Atoms(\n",
    "    numbers=structure[spk.properties.Z], positions=structure[spk.properties.R]\n",
    ")\n",
    "\n",
    "# convert atoms to SchNetPack inputs and perform prediction\n",
    "inputs = converter(atoms)\n",
    "results = best_model(inputs)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<schnetpack.data.atoms.ASEAtomsData object at 0x000001490E82D690>\n"
     ]
    }
   ],
   "source": [
    "temp = hessianData\n",
    "print(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
